
import pulp
import random
import numpy as np
import pandas as pd
import networkx

from src.constants import queries

def addLPVariable(key, variable_dict, lp_type='ILP'):
    """
    Creates an LP / ILP Decision variable and adds it to a list if it does not already exist

    Args:
        key (str): Variable name
        variable_dict (dict): Dict of existing variables
        varType (str, optional): Denotes type of optimization problem. Defaults to 'ILP'.
    """
    variable_type = {'ILP': 'Binary', 'LP': 'Continuous'}
    if not key in variable_dict:
        variable_dict[key] = pulp.LpVariable(key, lowBound = 0, upBound = 1, cat = variable_type[lp_type])

def computeWitnesses(query, database_instance):
    """    
    Perform joins to obtain witnesses given a database instance (tuples in set of table)

    Args:
        query (Query): A boolean conjunctive query described by the variables in each table
        database_instance (list): A list of tuples present in each table
    Returns:
        witnesses (DataFrame): The witnesses generated by the database instance under the given query
    """

    databaseInstanceDF = []
    query_body = query.query_body
    for (table_name, table_columns) in query_body:
        tableDF = pd.DataFrame(database_instance[table_name], columns=table_columns)
        filter_conflicting_columns =  tableDF.groupby(tableDF.columns,axis=1).agg(lambda x: x.eq(x.iloc[:, 0], axis=0).all(1))
        tableDF = tableDF[filter_conflicting_columns].reset_index(drop = True).groupby(tableDF.columns,axis=1).first()
        databaseInstanceDF.append(tableDF)

    witnesses = pd.DataFrame([0], columns=list(['key']))
    for table in databaseInstanceDF:
        table['key'] = 0
        witnesses = pd.merge(witnesses, table)
    witnesses = witnesses.drop('key', axis=1)

    if query.constants != None:
        # filter dataframe to match constants
        for constant in query.constants:
            witnesses = witnesses[witnesses[constant] == query.constants[constant]]
    return witnesses

def representWitnessAsList(query, witnesses):
    '''
    Represent the witnesses as a list of tuples

    Args:
        query (Query): A conjunctive query
        witnesses (DataFrame): The witnesses generated by the database instance under the given query
    '''
    # Convert each row to a list of tuples 
    query_body = query.query_body
    witnesses = witnesses.apply(lambda row: [table_name+ '_' +'_'.join([str(row[variable]) for variable in table_columns]) for (table_name, table_columns) in query_body], axis=1).values
    # Sort each witness in witnesses and remove duplicates
    witnesses = [sorted(list(set(witness))) for witness in witnesses]
    
    return witnesses

def representProjectionAsList(query, witnesses_df):
    '''
    Represent the projection as a list of witnesses

    Args:
        query (Query): A conjunctive query
        witnesses (DataFrame): The witnesses generated by the database instance under the given query
    '''
    # Convert each row to a list of tuples 
    query_head = query.head_vars
    projection_list = {}

    for _, witness in witnesses_df.iterrows():
        projection_key = 'p-'+'-'.join(sorted([variable+'_'+str(witness[variable]) for variable in query_head]))
        if projection_key not in projection_list:
            projection_list[projection_key] = []

        witness_key = 'w-'+'-'.join(sorted(list(set([table_name+ '_' +'_'.join([str(witness[variable]) for variable in table_columns]) for (table_name, table_columns) in query.query_body]))))
        projection_list[projection_key].append(witness_key)

    return projection_list
 

def performSemijoinReduction(query, database_instance):
    """
    Removes the tuples that do not participate in the query
    Args:
        query (list): A boolean conjunctive query described by the variables in each table
        database_instance (list): A list of tuples present in each table
    Returns:
        pruned_database_instance (DataFrame): Instance with only the tuples that participate in the query
    """
    witnesses = computeWitnesses(query, database_instance)
    pruned_database_instance = {table: pd.DataFrame() for table in database_instance.keys()}
    for tablename, table_vars in query:
        all_tuples_list = witnesses[table_vars]
        if len(pruned_database_instance[tablename]) == 0:
            pruned_database_instance[tablename] = all_tuples_list
        else:
            pruned_database_instance[tablename].columns = all_tuples_list.columns
            pruned_database_instance[tablename] = pd.concat([pruned_database_instance[tablename], all_tuples_list], axis=0, ignore_index=True)
    
    for table in database_instance.keys():
        pruned_database_instance[table] = pruned_database_instance[table].drop_duplicates()
        pruned_database_instance[table] = pruned_database_instance[table].values.tolist()

    
    return pruned_database_instance

def addTuples(query, step_size, domain_size, instance_data, bag_semantics = False, max_bag_size = 10):
    """
    Adds step_size new tuples to a given instance
    The new tuples are split over all tables in a random distribution.

    Args:
        query (list): A boolean conjunctive query described by the variables in each table
        step_size (int): Number of tuples to be added to the instance
        domain_size (int): Max domain size of resulting instance
        instance_data (list): Pre-existing instance including the database_instance and tuple_weights
        bag_semantics (bool, optional). Indicates if bag semantics or set semantics is used. Defaults to False.
        max_bag_size (int). Size of the largest bag possible
    """

    
    tables = set((table_name, len(table_variables)) for (table_name, table_variables) in query)

    numberOfTables = len(tables)
    if instance_data == None:
        database_instance = {t: set() for (t,_) in tables}
        tuple_weights = {}
    else:
        (database_instance, tuple_weights) = instance_data


    
    table_distribution = [0] * numberOfTables
    for _ in range(step_size):
        table_distribution[random.randint(0, numberOfTables - 1)] += 1

    index = 0
    for (table_name, arity) in tables:
        maxTuples = domain_size ** arity

        for _ in range(table_distribution[index]):
            if len(database_instance[table_name]) >= maxTuples:
                break
            else:
                t = tuple(np.random.randint(0, domain_size, size = arity))
                database_instance[table_name].add(t)
        index += 1

    if bag_semantics:
        
        
        for (table_name, arity) in tables:
            table = database_instance[table_name]
            for t in table:
                tuple_key = table_name + '_' + '_'.join(str(x) for x in t)
                if tuple_key not in tuple_weights:
                    tuple_weights[tuple_key] = np.random.randint(1, max_bag_size)
            
    return (database_instance, tuple_weights)

def pickRandomTuple(query, database_instance, resp_table):
    """
    For a given query and database instance, a random tuple from a given table

    Args:
        query (list): A boolean conjunctive query described by the variables in each table
        database_instance (dict): A collection of database table
        resp_table (str): table name from which to create database instance

    Returns:
        A key for a tuple randomly chosen from the table.
    """

    witnesses = computeWitnesses(query, database_instance)
    if len(witnesses) == 0:
        return None
    resp_table_in_query = list(filter(lambda x: x[0] == resp_table,query))
    
    resp_table_variables = random.choice(resp_table_in_query)[1]
    possible_resp_tuples = witnesses[resp_table_variables].drop_duplicates().values.tolist()
    resp_tuple = random.choice(possible_resp_tuples)
    resp_tuple = resp_table + "_" + "_".join(map(str, resp_tuple))

    return resp_tuple


def get_existential_cc(query):
    '''
    Constructs the existential graph of the query and generates connected components

    Parameters:
    query (Query): The query for which the existential graph is to be constructed

    Returns:
    connected_components (list): A list of connected components
    '''

    existential_graph = networkx.Graph()
    for i, _ in enumerate(query.query_body):
        existential_graph.add_node(i)

    all_variables = set()
    for _, table_vars in query.query_body:
        all_variables.update(table_vars)
    
    existential_variables = all_variables - set(query.head_vars)

    # Add edge between two nodes if they share an existential variable
    for i in range(len(query.query_body)):
        for j in range(i+1, len(query.query_body)):
            if len(set(query.query_body[i][1]).intersection(query.query_body[j][1]).intersection(existential_variables)) > 0:
                existential_graph.add_edge(i, j)

    connected_components_index = list(networkx.connected_components(existential_graph))
    connected_components = [[query.query_body[i] for i in cc] for cc in connected_components_index]
    
    return connected_components


def isSingleton(query):
    '''
    A query is singleton if there is a relation that dominates all other relations and either is a subset or superset of the head variables
    
    Parameters:
    query (Query): The query we check for singleton property

    Returns:
    bool: True if query is singleton
    table_name: The name of the singleton table
    '''

    for table_name, table_vars in query.query_body:

        if set(table_vars).issubset(set(query.head_vars)) or set(table_vars).issuperset(set(query.head_vars)):
            pass
        else:
            continue

        dominates = True
        for other_table_name, other_table_vars in query.query_body:
            if not set(table_vars).issubset(other_table_vars):
                dominates = False
                break
        if dominates:
            return True, table_name
                
    return False, None

def isConnected(query):
    '''
    A query is connected if the variable graph is connected
    
    Parameters:
    query (Query): The query we check for connected property
    
    Returns:
    bool: True if query is connected
    list: A list of connected components
    '''

    variable_graph = networkx.Graph()
    for i in range(len(query.query_body)):
        variable_graph.add_node(i)

    for i in range(len(query.query_body)):
        for j in range(i+1, len(query.query_body)):
            if len(set(query.query_body[i][1]).intersection(query.query_body[j][1])) > 0:
                variable_graph.add_edge(i, j)

    connected_components_index = list(networkx.connected_components(variable_graph))
    connected_components = [[query.query_body[i] for i in cc] for cc in connected_components_index]
    
    return len(connected_components)==1, connected_components

def hasUniversalAttribute(query):
    '''
    A query has a universal attribute if there is a head variable that is in all relations

    Parameters:
    query (Query): The query we check for universal attribute property
    
    Returns:
    bool: True if query has universal attribute
    Query: List of all universal attributes
    '''

    all_variables = set()
    for _, table_vars in query.query_body:
        all_variables.update(table_vars)

    universal_attributes = set(query.head_vars)
    for (table_name, table_vars) in query.query_body:
        universal_attributes = universal_attributes.intersection(set(table_vars))

    return len(universal_attributes) > 0, universal_attributes
    